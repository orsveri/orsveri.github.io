[
  {
    "objectID": "posts/topic_statistics/1_intro/index.html",
    "href": "posts/topic_statistics/1_intro/index.html",
    "title": "Introduction to statistics",
    "section": "",
    "text": "About this series\n\n\n\nThis is an article from the series about statistics (and probability theory). The motivation behind this series is to retain knowledge while taking courses, reading books and articles. I write articles about something that I missed before, found interesting or possibly useful to me in the future. These articles do not constitute a course, they are just a collection of my personal study notes or summaries on different topics related to statistics."
  },
  {
    "objectID": "posts/topic_statistics/1_intro/index.html#statistics-and-probability-theory",
    "href": "posts/topic_statistics/1_intro/index.html#statistics-and-probability-theory",
    "title": "Introduction to statistics",
    "section": "Statistics and probability theory",
    "text": "Statistics and probability theory\nTypically, probability theory and statistics go side by side in the curriculum, so students do not always understand the difference between the subjects. Indeed, we constantly use them together, then why do we need to separate them at all?\nSome prefer to explain the difference through model and data:\n\nIn probability theory, we take a model (e.g. fair coin) and predict how likely it is to get some event with this model (e.g. tails 5 times in a row).\nIn statistics, we take data (e.g. number of heads and tails in 100 tosses) and infer what is the model behind this data (e.g. biased coin).\n\nThis may give the impression that probability theory and statistics solve problems that are inverse to each other. But this is not exactly the case.\n\n\n\nFigure 1: Statistics VS Probability\n\n\nProbability theory is the branch of mathematics that deals with the study of randomness, uncertainty, and the likelihood of events occurring. The key concepts in probability theory include events and random variables, probability laws and probability distributions.\nStatistics is the branch of mathematics that deals with collecting, analysing, and interpreting data. It involves the use of mathematical and computational tools to not only make inferences from data, but also to test hypotheses and predict future events. Statistics provides ways to design efficient experiments that eliminate time-consuming trial and error. Key concepts of statistics include data, variability, confidence intervals, sampling, inference.\nSo they deal with completely different problems and use different methods and tools.\n\n\n\nKey subfields \n\n\n\n\n\n\nProbability theory\nStatistics\n\n\n\n\nStochastic processes: This subfield deals with the study of random processes that evolve over time. Examples include random walks, Brownian motion, and Markov processes.\nDescriptive statistics: This subfield deals with the organisation, summarisation, and graphical representation of data, such as mean, median, mode, standard deviation, and histograms.\n\n\nInformation theory: This subfield deals with the quantification and transmission of information in the presence of noise and uncertainty. Key concepts include entropy, mutual information, and channel capacity.\nInferential statistics: This subfield deals with making inferences about a population based on a sample of data, using tools such as hypothesis testing, confidence intervals, and regression analysis.\n\n\nProbability distributions: This subfield deals with the study of probability distributions, which describe the probabilities of different outcomes in a random variable.\nExperimental design: This subfield deals with the planning and analysis of controlled experiments, including randomisation, blocking, and factorial designs.\n\n\nRandom matrix theory: This subfield deals with the study of matrices with random elements. It has applications in physics, engineering and other fields.\nTime series analysis: This subfield deals with the modeling and analysis of data that vary over time, such as stock prices, weather patterns and economic indicators.\n\n\n\n\n\nNevertheless, statistics and probability theory are closely related fields, with some subjects tightly connected. For example, Bayesian statistics.\n\nBayesian statistics is considered a subfield of probability theory because it uses probability theory to model and quantify uncertainty in statistical inference. In contrast to traditional frequentist statistics, which assumes that probabilities are based on frequencies of events in long-run experiments, Bayesian statistics assigns probabilities to events based on prior knowledge and data.\nIn Bayesian statistics, prior beliefs about the probability of an event are combined with new data to update the probability estimates. The resulting probability distribution is then used to make statistical inferences and predictions. Bayesian statistics also allows for the incorporation of expert knowledge and subjective opinions into the analysis.\n\nIn what follows, I will explore some topics in statistics and probability theory in order to close the gaps left after my university studies and to better understand both these fields."
  },
  {
    "objectID": "posts/topic_statistics/1_intro/index.html#descriptive-statistics",
    "href": "posts/topic_statistics/1_intro/index.html#descriptive-statistics",
    "title": "Introduction to statistics",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\nPeople prefer to look at pictures or figures rather than at numbers.\n\nThe two most important functions of descriptive statistics:\n\ncommunicate information\nsupport reasoning about data\n\nChoosing an appropriate representation, the first thing you need to look at is whether your data is categorical or numerical.\n\nCategorical (qualitative) data\nCategorical data usually represents data divided into groups (or categories) and can be visualised using a pie chart and dot plot.\n\n\nCode\n# Data from: https://www.statista.com/statistics/301726/mayonnaise-usage-frequency-in-the-uk/\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport pandas as pd\nimport numpy as np\n\n\ndef dot_plot(ax, x_values, y_values, color):\n    ax.plot(\n        x_values,\n        y_values,\n        marker='$\\\\bigoplus$', \n        markersize=12, \n        linestyle='None',\n        color=color\n    )\n    \ndef set_axis(ax, title, xlabel):\n    ax.set_xlabel(xlabel)\n    ax.set_title(title)\n    ax.set_axisbelow(True)\n    ax.grid(which='major', linestyle='-', linewidth='0.5', color='grey')\n    ax.minorticks_on()\n    ax.grid(which='minor', linestyle=':', linewidth='0.5', color='grey')\n\n# data\nmayonnaise_consumption = pd.DataFrame({\n    'Group': [\n        'Never', 'Less than \\nonce a week', 'Once a week', '2-3 times \\na week', 'Once a day \\nor more'\n    ], \n    'Number': [798250, 6575180, 4471460, 6506960, 1503030]\n})\ntotal_people = sum(mayonnaise_consumption['Number'])\n\n# Prepare subplots\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(13, 3), gridspec_kw={'width_ratios': [3, 1, 2]}, num='dot_plot')\nfig.suptitle('Mayonnaise usage frequency in the UK (2021)', fontsize=14)\nfig.subplots_adjust(top=0.8)\n\n# Pie chart\nwedges, texts, autotexts = ax1.pie(\n    mayonnaise_consumption['Number'], \n    autopct='%1.1f%%',\n    textprops=dict(color='w')\n)\nplt.setp(autotexts, size=12, weight=\"bold\")\nax1.set_title('Pie chart')\n# pie chart legend\nax2.axis(\"off\")\nax2.legend(\n    wedges, \n    mayonnaise_consumption['Group'],\n    title=\"Frequency\",\n    bbox_to_anchor=(0.4,1)\n)\n\n# Dot plot (using scatter plot)\ndot_plot(ax=ax3, x_values=mayonnaise_consumption['Number'] / total_people * 100, y_values=mayonnaise_consumption['Group'], color='k');\nset_axis(ax=ax3, title='Dot plot', xlabel='Percent')\n\nplt.show('dot_plot')\nfig.clear()\nplt.close(fig)\n\n\n\n\n\nAlso we can use a variant of bar chart, very similar to dot plot: instead of representing values with markers, we just draw bars. When we have several variables to visualise, one graph type may be better than the other in terms of visually separating variables from each other. Let’s make up some extra data to demonstrate this case:\n\n\nCode\nmayonnaise_consumption1_number = mayonnaise_consumption['Number'] / total_people * 100\n\n# made-up data\nmayonnaise_consumption2_number = [1008250, 6575180, 4671460, 6506960, 1803030]\ntotal_people2 = sum(mayonnaise_consumption2_number)\nmayonnaise_consumption3_number = [838250, 6875180, 4171460, 6506960, 1103030]\ntotal_people3 = sum(mayonnaise_consumption3_number)\nmayonnaise_consumption2_number = [n / total_people2 * 100 for n in mayonnaise_consumption2_number]\nmayonnaise_consumption3_number = [n / total_people3 * 100 for n in mayonnaise_consumption3_number]\n                                           \nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 3), gridspec_kw={'width_ratios': [3, 3, 1], 'wspace':0}, sharex=True, sharey=True, num='dot_n_bar')\nfig.suptitle('Mayonnaise usage frequency (2021)', fontsize=14)\nfig.subplots_adjust(top=0.8)\n\n# Dot plot (using scatter plot)\ndot_plot(ax=ax1, x_values=mayonnaise_consumption1_number, y_values=mayonnaise_consumption['Group'], color='b')\ndot_plot(ax=ax1, x_values=mayonnaise_consumption2_number, y_values=mayonnaise_consumption['Group'], color='g')\ndot_plot(ax=ax1, x_values=mayonnaise_consumption3_number, y_values=mayonnaise_consumption['Group'], color='r')\nset_axis(ax=ax1, title='Dot plot', xlabel='Percent')\n\n# Bar plot \ny_pos = np.arange(len(mayonnaise_consumption1_number))\nset_axis(ax=ax2, title='Bar plot', xlabel='Percent')\nax2.barh(y_pos-0.3, mayonnaise_consumption1_number, height=0.2, align='edge', color='b')\nax2.barh(y_pos-0.1, mayonnaise_consumption2_number, height=0.2, align='edge', color='g')\nax2.barh(y_pos+0.1, mayonnaise_consumption3_number, height=0.2, align='edge', color='r')\n\n# Legend\nax3.axis(\"off\")\nlegend = ax3.legend(\n    wedges, \n    ['the UK', 'Country 2', 'Country 3'],\n    title=\"Countries\",\n    bbox_to_anchor=(1.4, 1)\n)\nlegend.legendHandles[0].set_color('b')\nlegend.legendHandles[1].set_color('g')\nlegend.legendHandles[2].set_color('r')\n\nplt.show('dot_n_bar')\nfig.clear()\nplt.close(fig)\n\n\n\n\n\n\n\nNumerical (quantitative) data\nNumerical data is data in the form of numbers and it allows us to perform arithmetic operations on them. Numerical data should be represented on a number line as the ordering and distance between the numbers hold significant information. Most suitable graphical representations for numerical data are (vertical) histogram (bar chart), scatter plot and box plot.\nA scatter plot is the most obvious way to visualise data. It makes the most sense with 2D-3D data, when we need to determine the relation between two or three variables in a dataset. Third dimension can be added as a colour of plotted points.\n\n\nCode\nimport seaborn as sns\n\niris = sns.load_dataset('iris')\ndiamonds = sns.load_dataset('diamonds')\n\nfig, axs = plt.subplots(2, 2, figsize=(11, 6), gridspec_kw={'width_ratios': [1, 1], 'wspace':0.2, 'hspace':0.5}, num='scatter')\nfig.suptitle('Scatter plot examples', fontsize=14)\nfig.subplots_adjust(top=0.8)\n\n# iris\nsns.scatterplot(ax=axs[0,0], x=\"sepal_length\", y=\"sepal_width\", hue=\"species\", linewidth=0, data=iris, legend=False).set_title('Iris species by sepal width and length')\nsns.scatterplot(ax=axs[0,1], x=\"sepal_length\", y=\"petal_length\", hue=\"species\", linewidth=0, data=iris).set_title('Iris species by sepal and petal lengths')\nsns.scatterplot(ax=axs[1,1], x=\"petal_width\", y=\"petal_length\", hue=\"species\", linewidth=0, data=iris, legend=False).set_title('Iris species by petal width and length')\n\nsns.scatterplot(ax=axs[1,0], x=\"carat\", y=\"depth\", hue=\"price\", data=diamonds).set_title('Diamond price by its carat (weight) and depth (height)')\n\nplt.show('scatter')\nfig.clear()\nplt.close(fig)\n\n\n\n\n\nA histogram can be seen as a type of bar chart, but it has the following key differences:\n\nA histogram shows distribution, so ordering of the value axis is important. With categorical data, order of bars or columns in a bar chart doesn’t matter.\nIn a histogram, data points are grouped into bins (small value ranges) based on their value and then counted. In a bar chart, data points for each category are counted separately, so there can be space between bars (axis is not numerical). Histogram doesn’t have space between bars, because it shows a continuous range of values (axis is numerical).\n\nThere are different types of histograms:\n\nA frequency histogram shows number of occurrences (observations) in each bin.\nIn a density histogram, the total area of all bars (blocks) adds to 1. The area of each bar corresponds to the percentage (so it can show a probability). The height of each bar is probability density, calculated using this formula:\n\n\\[\\begin{equation} \\frac{bin\\_observations}{ total\\_observations * bin\\_width}. \\end{equation}\\]\nHistograms can have various shapes: uniform, unimodal (symmetric), bimodal (two peaks), multimodal (many peaks) and skewed (one peak, but off center). If the histogram is symmetric or uniform, then the mean and the median of a distribution are approximately the same. In case it is skewed, it’s better to use the median.\n\n\nCode\n# Histograms\n\ndef set_histaxis(ax, title, xlabel, ylabel):\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_title(title)\n    ax.autoscale()\n    ax.grid(which='major', linestyle='-', linewidth='0.5', color='grey')\n    \n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 3), gridspec_kw={'width_ratios': [3, 3, 3], 'wspace':0.3}, sharex=False, num='histograms')\nfig.suptitle('Waiting time between eruptions for the Old Faithful geyser \\n in Yellowstone National Park, Wyoming, USA.', fontsize=14)\nfig.subplots_adjust(top=0.7)\n\ndf = sns.load_dataset('geyser')\nvalues, counts = np.unique(df['waiting'], return_counts=True)\nv_range = np.min(values), np.max(values)\nc_range = np.min(counts), np.max(counts)\n\n# Bar plot\nsns.countplot(ax=ax1, data=df, x='waiting')\nset_histaxis(ax=ax1, title='Bar plot', xlabel='Time (s)', ylabel='Counts')\nax1.set_xticks(ax1.get_xticks()[::5])\n\n# Histograms\nbins = 12\n\n# Freq histogram\nsns.histplot(ax=ax2, data=df, x='waiting', stat='count', bins=bins)\nset_histaxis(ax=ax2, title=f'Frequency histogram, {bins} bins', xlabel='Time (s)', ylabel='Frequency')\n\n# Density histogram\nsns.histplot(ax=ax3, data=df, x='waiting', stat='density', bins=bins)\nset_histaxis(ax=ax3, title=f'Density histogram, {bins} bins', xlabel='Time (s)', ylabel='Density')\n\nplt.show('histograms')\nfig.clear()\nplt.close(fig)\n\n\n\n\n\nA box plots displays five-number summary of a distribution: the minimum, the maximum, the median and the first and third quartiles. When we have several variables, visualising their distributions using a box plot (box and whisker plot) allows us to get all the essential information and compare them quickly and easily.\n\n\n\nFigure 2: Box and whisker plot\n\n\nThere’s another type of plot, similar to box plot – violin plot. In addition to five-number summary, it also shows kernel density plot. In simple words, kernel density plot is an estimation of the probability density function for a distribution. We can get an estimated probability density function for a distribution by interpolating or smoothing its density histogram.\n\n\nCode\n# Load the example tips dataset\nexercise = sns.load_dataset('exercise')\npalette = \"pastel\"\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw={'width_ratios': [1, 1], 'wspace':0}, sharex=True, sharey=True, num='boxplot')\n\nsns.boxplot(ax=ax1, data=exercise, x='kind', y='pulse', hue='diet', width=0.3, palette=palette)\nax1.get_legend().remove()\nax1.set_title('Box plot')\nax1.set_xlabel('Activity')\nax1.set_ylabel('Pulse')\nax1.grid(which='major', linestyle='-', linewidth='0.5', color='grey')\n\nsns.violinplot(ax=ax2, data=exercise, x=\"kind\", y='pulse', hue='diet', split=False, inner='box', linewidth=1, palette=palette)\nax2.set_title('Violin plot')\nax2.set_xlabel('Kind')\nax2.grid(which='major', linestyle='-', linewidth='0.5', color='grey')\nax2.yaxis.label.set_visible(False)\nax2.legend(title='Diet', bbox_to_anchor=(0.3, 1))\n\nplt.show('boxplot')\nfig.clear()\nplt.close(fig)\n\n\n\n\n\n\n\nThe principle of small multiples\nThe principle of small multiples is a visualization technique in data analysis and design where multiple similar plots or charts are presented together in a grid or series, each one highlighting a different aspect or variation of the same data set. The idea is that by presenting information in a consistent and easily comparable format, it becomes easier for the viewer to discern patterns and trends within the data. Small multiples can also help to simplify complex data sets by breaking them down into more digestible, bite-sized chunks.\n\nThe principle of small multiples was first introduced by the data visualization expert Edward Tufte in his book “The Visual Display of Quantitative Information”, which was published in 1983. Tufte is a well-known expert in the field of information design and has made significant contributions to the field of data visualization.\n\n\n\nCode\nflights = sns.load_dataset('flights')\nylim = flights['passengers'].min()-20, flights['passengers'].max()+20\nsns.set_style(\"ticks\",{'axes.grid' : True})\n\n# Initialize a grid of plots with an Axes for each walk\ngrid = sns.FacetGrid(data=flights, col='year', hue=\"year\", palette='pastel', sharex=True, col_wrap=4, height=2)\ngrid.refline(y=flights['passengers'].median())\n\n# Draw a line plot to show the trajectory of each random walk\ngrid.map(plt.plot, \"month\", \"passengers\", marker=\"o\")\n\n# Adjust the tick positions and labels\ngrid.set(ylim=ylim, xticks=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n\nfor ax in grid.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(90)\n        label.set_size(9)\n\n# Adjust the arrangement of the plots\ngrid.fig.tight_layout(w_pad=1)"
  },
  {
    "objectID": "posts/topic_statistics/1_intro/index.html#inferential-statistics-statistical-inference",
    "href": "posts/topic_statistics/1_intro/index.html#inferential-statistics-statistical-inference",
    "title": "Introduction to statistics",
    "section": "Inferential statistics (Statistical inference)",
    "text": "Inferential statistics (Statistical inference)\nTODO - this section will be updated later.."
  },
  {
    "objectID": "posts/topic_statistics/3_estd_laws/index.html",
    "href": "posts/topic_statistics/3_estd_laws/index.html",
    "title": "Expected value, sampling distribution, the central limit theorem",
    "section": "",
    "text": "About this series\n\n\n\nThis is an article from the series about statistics (and probability theory). The motivation behind this series is to retain knowledge while taking courses, reading books and articles. I write articles about something that I missed before, found interesting or possibly useful to me in the future. These articles do not constitute a course, they are just a collection of my personal study notes or summaries on different topics related to statistics."
  },
  {
    "objectID": "posts/topic_statistics/3_estd_laws/index.html#expected-value",
    "href": "posts/topic_statistics/3_estd_laws/index.html#expected-value",
    "title": "Expected value, sampling distribution, the central limit theorem",
    "section": "Expected value",
    "text": "Expected value\nIn simple terms, the expected value is the average value that we expect to get if we repeat a random process many times. It is a measure of the central tendency of a random variable. So, to calculate expected value, we need to understand what is a random variable in our case.\nNotation\n\nRandom variable: \\(X\\)\nExpected value of \\(X\\): \\(E[X]\\) (as functional), \\(E(X)\\) (as function) or \\(EX\\).\n\n\n\n\nExpected value formulas \n\n\n\n\n\n\n\n\nBinomial\nMultinomial\nContinuous\nArbitrary\n\n\n\\(EX = P(x) * X\\)\n\\(EX = \\displaystyle\\sum_{i}^{N} P(x_i) * x_i\\)\n\\(EX = \\displaystyle\\int_{-\\infty}^\\infty x f(x) dx\\)\n\\(EX = \\displaystyle\\sum_{i} g(x) * f(x)\\)\n\n\nIf probability of winning a lottery (getting a prize) is \\(0.1\\), and you buy \\(20\\) tickets, then the expected value (number of prizes) is \\(0.1*20 = 2\\), i.e. \\(2\\) prizes.\nblabla\nblabla\nblabla"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index2.html",
    "href": "index2.html",
    "title": "listing",
    "section": "",
    "text": "Expected value, sampling distribution, the central limit theorem\n\n\nStatistics notes - part 3\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2023\n\n\norsveri\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory basics\n\n\nStatistics notes - part 2\n\n\n\n\nstatistics\n\n\nprobability theory\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\norsveri\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to statistics\n\n\nStatistics notes - part 1\n\n\n\n\nstatistics\n\n\nprobability theory\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\norsveri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/topic_statistics/2_prob_basics/index.html",
    "href": "posts/topic_statistics/2_prob_basics/index.html",
    "title": "Probability theory basics",
    "section": "",
    "text": "About this series\n\n\n\nThis is an article from the series about statistics (and probability theory). The motivation behind this series is to retain knowledge while taking courses, reading books and articles. I write articles about something that I missed before, found interesting or possibly useful to me in the future. These articles do not constitute a course, they are just a collection of my personal study notes or summaries on different topics related to statistics."
  },
  {
    "objectID": "posts/topic_statistics/2_prob_basics/index.html#outcomes-and-events",
    "href": "posts/topic_statistics/2_prob_basics/index.html#outcomes-and-events",
    "title": "Probability theory basics",
    "section": "Outcomes and events",
    "text": "Outcomes and events\nIn statistics, observational units are people, things, systems, etc., for which data is observed. Variables are characteristics or quantities that represent attributes of observational units. For example, if a hive of bees is an observational unit, then variables can include the type of bees, the age of the hive, the number of hive frames, the defensiveness of the bees, etc.\nIn probability theory, an outcome is the result of some random situation, experiment or phenomenon. It can be a coin flip, a soccer match, a new neighbour, or the weather tomorrow. The sample space is the collection of all possible outcomes of a random phenomenon. But in practice, it is often infeasible or even impossible to list all possible outcomes, so instead, we work with events. An event is a specific combination of outcomes or a subset of the sample space that we are interested in. For example, “Team A wins the match with team B”, “The temperature is exactly 20°C”, “My new neighbour is taller than me, has any number of kids and one cat” or “Getting 5 tails out of 7 coin flips and the first flip results in tail”.\nBecause events are sets, we can use basic set operations: unions (or), intersections (and) and complements (not). Because the probability of the union of all outcomes in sample space is always equal to 1, if we know the probability of some event, we can easily find its complementary event without calculating the probabilities of all remaining outcomes.\nIn probability theory, outcomes are somewhat similar to observational units in statistics in that sense that typically they both aren’t defined strictly. More often we work with events instead of outcomes and variables instead of observational units."
  },
  {
    "objectID": "posts/topic_statistics/2_prob_basics/index.html#random-variables",
    "href": "posts/topic_statistics/2_prob_basics/index.html#random-variables",
    "title": "Probability theory basics",
    "section": "Random variables",
    "text": "Random variables\nRandom variable, also known as stochastic variable, describes the numerical outcome of a random experiment or phenomenon. It encompasses all possible outcomes in some form. Random variables can be divided into two categories:\n\ndiscrete random variable - can take on a finite number of distinct values. Example: the number of kids in a family.\ncontinuous random variable - can take on an infinite number of possible values (from an interval). Example: the weight of a potato."
  },
  {
    "objectID": "posts/topic_statistics/2_prob_basics/index.html#probability-distribution",
    "href": "posts/topic_statistics/2_prob_basics/index.html#probability-distribution",
    "title": "Probability theory basics",
    "section": "Probability distribution",
    "text": "Probability distribution\nProbability distribution is a mathematical function that describes the likelihood of different outcomes or events. There are several such functions:\n\nPDF: Probability density function - describes the probability distribution of a continuous random variable.\nPMF: Probability mass function - describes the probability distribution of a discrete random variable.\nCDF: Cumulative distribution function - gives the cumulative probability distribution for a random variable (discrete or continuous). the CDF provides the probability that a random variable is less than or equal to a specific value. It is defined for both discrete and continuous random variables.\n\n\nDiscrete distributions\nA discrete distribution models discrete random variables, i.e. variables with a countable number of possible outcomes.\n\nBinomial. The binomial distribution describes the number of successes in a fixed number of trials with two possible outcomes. For example, the number of tails in 100 coin tosses.\nBernoulli. Models a single trial with two possible outcomes: success or failure. It is often used to represent situations where there is a binary event, and we want to calculate the probability of observing a specific outcome. For example, the probability of drawing a red card from a deck.\nGeometric. Models the number of trials needed to achieve the first success in a sequence of independent Bernoulli trials, where each trial has two possible outcomes (success or failure) and the probability of success remains the same for each trial. The geometric distribution is commonly used in various applications, such as modelling the number of attempts until an event occurs, analyzing waiting times, and estimating probabilities in scenarios involving repeated trials with a fixed probability of success.\nPoisson. Models the number of events occurring in a fixed interval of time or space, given the average rate of occurrence. It is used when events happen randomly and independently over a continuous interval. Unlike binomial distribution, we don’t have any successes or failures, we only model how many events occurred. The Poisson distribution is often used in situations where events occur with a low probability but at a high rate, such as modelling the number of phone calls received at a call centre in a given time period, the number of customers arriving at a store per hour, or the number of accidents on a particular stretch of road in a day.\n\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nfrom scipy.stats import binom, geom, bernoulli, poisson\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(\n    4, 2, figsize=(11, 12), num='discrete', \n    gridspec_kw={'width_ratios': [2, 1], 'wspace':0.1, 'hspace':0.4}\n)\nfig.suptitle(\n    'Discrete distributions examples, \\nprobability mass functions (PMFs) and \\\n    cumulative distribution functions (CDFs)', \n    fontsize=14\n)\nfig.subplots_adjust(top=0.9)\n\n# Binomial\n#ax = axs[0,0]\np = 0.7  # the probability of success\nn = 100  # the number of trials\nx = np.arange(binom.ppf(0.001, n, p), binom.ppf(0.999, n, p)).astype(int)\ny = binom.pmf(x, n, p)\ny2 = binom.cdf(x, n, p)\nsns.barplot(ax=axs[0,0], x=x, y=y).set_title('Binomial distribution PMF')\nsns.lineplot(ax=axs[0,1], x=x, y=y2).set_title('Binomial distribution CDF')\nax.set_xticks(axs[0,0].get_xticks()[::3])\n\n# Bernoulli\nax = axs[1,0]\np = 0.3  # the probability of success (e.g. obserivng a specific outcome)\nx = [0., 1.]  # 0 is failure and 1 is success\nx_plot = ['failure', 'success']  # just to hve a nicer plot\ny = bernoulli.pmf(x, p)\ny2 = bernoulli.cdf(x, p)\nax.plot(x, y, 'bo', ms=8, label='bernoulli pmf')\nax.vlines(x_plot, 0, y, colors='b', linestyles='-', lw=2, label='frozen pmf')\nax.set_title('Bernoulli distribution PMF')\nax.set_xlabel('Outcome')\nsns.lineplot(ax=axs[1,1], x=x, y=y2).set_title('Bernoulli distribution CDF')\n\n# Geometric\nx = np.arange(geom.ppf(0.001, p), geom.ppf(0.999, p)).astype(int)\ny = geom.pmf(x, p)\ny2 = geom.cdf(x, p)\nsns.barplot(ax=axs[2,0], x=x, y=y).set_title('Geometric distribution PMF')\nsns.lineplot(ax=axs[2,1], x=x, y=y2).set_title('Geometric distribution CDF')\n\n# Poisson\nax = axs[3,0]\nmu = 30  # the average rate of events occurring in the given interval\nx = np.arange(poisson.ppf(0.001, mu), poisson.ppf(0.999, mu)).astype(int)\ny = poisson.pmf(x, mu)\ny2 = poisson.cdf(x, mu)\nsns.barplot(ax=ax, x=x, y=y).set_title('Poisson distribution PMF')\nax.set_xticks(ax.get_xticks()[::3])\nsns.lineplot(ax=axs[3,1], x=x, y=y2).set_title('Poisson distribution CDF')\n\nplt.show('discrete')\nfig.clear()\nplt.close(fig)\n\n\n\n\n\n\n\nContinuous distributions\nMost popular examples of continuous distributions are:\n\nUniform. All values within a specified range have equal probability. It is often used when there is no preference or bias towards any particular value in the range.\nNormal or Gaussian. Characterized by a bell-shaped curve. It is used to model many natural phenomena, such as heights, weights, and measurement errors.\nExponential. This distribution models the time between events in a Poisson process, where events occur randomly and independently at a constant average rate.\nGamma. The gamma distribution is a versatile distribution that is often used to model waiting times, durations, and survival times. It is a continuous analogue of the negative binomial distribution.\n\nNote that the exponential distribution is a specific case of the gamma distribution with a shape parameter of 1, the gamma distribution offers more flexibility in modelling various scenarios by allowing different shapes and scales. The choice between these distributions depends on the specific characteristics and requirements of the data being analyzed.\n\n\nCode\nfrom scipy.stats import uniform, norm, expon, gamma\n\nfig, axs = plt.subplots(\n    4, 2, figsize=(11, 12), num='continuous', \n    gridspec_kw={'width_ratios': [2, 1], 'wspace':0.1, 'hspace':0.4}\n)\nfig.suptitle(\n    'Continuous distributions examples, \\nprobability density functions (PDFs) and \\\n    cumulative distribution functions (CDFs)', \n    fontsize=14\n)\nfig.subplots_adjust(top=0.9)\n\nx = np.linspace(-3, 3, 100)\n\n# Uniform\ny = uniform.pdf(x, loc=-1, scale=3)  # uniform probability on [loc, loc+scale]\ny2 = uniform.cdf(x, loc=-1, scale=3)\nsns.lineplot(ax=axs[0,0], x=x, y=y, legend=True).set_title('Uniform distribution PDF')\nsns.lineplot(ax=axs[0,1], x=x, y=y2, legend=True).set_title('Uniform distribution CDF')\n\n# Normal\ny = norm.pdf(x)\ny2 = norm.cdf(x)\nsns.lineplot(ax=axs[1,0], x=x, y=y, legend=True).set_title('Normal distribution PDF')\nsns.lineplot(ax=axs[1,1], x=x, y=y2, legend=True).set_title('Normal distribution CDF')\n\nx = np.linspace(-1, 8, 100)\n\n# Gamma\nfor a in (0.8, 1.4, 2.1, 5.):\n    y = gamma.pdf(x, a=a)\n    y2 = gamma.cdf(x, a=a)\n    sns.lineplot(ax=axs[2,0], x=x, y=y, label=f'a={a}').set_title('Gamma distribution PDF')\n    sns.lineplot(ax=axs[2,1], x=x, y=y2, label=f'a={a}').set_title('Gamma distribution CDF')\n\n# Exponential\ny = expon.pdf(x)\ny2 = expon.cdf(x)\nsns.lineplot(ax=axs[3,0], x=x, y=y, legend=True).set_title('Exponential distribution PDF \\n= Gamma(a=1.)')\nsns.lineplot(ax=axs[3,1], x=x, y=y2, legend=True).set_title('Exponential distribution CDF \\n= Gamma(a=1.)')\n\nplt.show('continuous')\nfig.clear()\nplt.close(fig)\n\n\n\n\n\n\n\nPractical notes: KDE vs PDF\nKernel Density Estimation is a non-parametric statistical technique used to estimate the probability density function (PDF) of a continuous random variable based on a set of observed data points. The goal of KDE is to create a smooth estimate of the underlying distribution of the data.\nIn KDE, a kernel function is placed at each data point, and these kernels are then combined to create a smooth curve that approximates the PDF. The kernel function is usually a smooth and symmetric function, such as the Gaussian (normal) distribution.\nBelow is an example from (sci-learn tutorial)[https://scikit-learn.org/stable/modules/density.html#kernel-density-estimation]\n\n\nCode\nfrom sklearn.neighbors import KernelDensity\n\nN = 100\nkernel3 = 'epanechnikov'\nbandwidth = 0.5\n\nx = np.linspace(-5, 10, 1000)\nx_sample = np.concatenate((np.random.normal(0, 1, int(0.3 * N)),\n                    np.random.normal(5, 1, int(0.7 * N))))[:, np.newaxis]\n\nfig, ax = plt.subplots(figsize=(11, 6), num='kde')\ntrue_density = (0.3 * norm(0, 1).pdf(x) + 0.7 * norm(5, 1).pdf(x))\nax.fill(x[:], true_density, fc='black', alpha=0.2, label='input distribution')\n\nfor kernel in [kernel3, 'tophat', 'gaussian']:\n    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(x_sample)\n    log_dens = kde.score_samples(x[:, np.newaxis])\n    ax.plot(x, np.exp(log_dens), '-',\n            label=\"kernel = '{0}'\".format(kernel))\n\n#ax.set_xlim(-4, 9)\n#ax.set_ylim(-0.02, 0.4)\n\nax.text(6, 0.28, f'{N} points, bandwidth {bandwidth}')\nax.legend(loc='upper left', prop={'size': 12})\n\nplt.show('kde')\nfig.clear()\nplt.close(fig)\n\n\n\n\n\n\n\nCode\n# TODO: add parameter sliders for interactivity\nimport numpy as np\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.neighbors import KernelDensity\n\nN = 100\nbandwidth = 0.5\n\nx = np.linspace(-5, 10, 1000)\nx_sample = np.concatenate((np.random.normal(0, 1, int(0.3 * N)),\n                    np.random.normal(5, 1, int(0.7 * N))))[:, np.newaxis]\ntrue_density = (0.3 * norm(0, 1).pdf(x) + 0.7 * norm(5, 1).pdf(x))\n\nfig = make_subplots(specs=[[{\"secondary_y\": False}]])\nfig.add_trace(\n    go.Scatter(x=x[:], y=true_density, fill='tozeroy', name='input distribution', line=dict(color='gray'))\n)\n\nN_steps = np.arange(20, 1000, 60)\nbandwidth_steps = (0.05, 0.1, 0.5, 1, 3)\nL = len(N_steps) + len(bandwidth_steps)\n\n\nfor kernel in ['tophat', 'gaussian', 'epanechnikov']:\n    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(x_sample)\n    log_dens = kde.score_samples(x[:, np.newaxis])\n    for n in N_steps:\n        for b in bandwidth_steps: \n            fig.add_trace(go.Scatter(visible=False, x=x, y=np.exp(log_dens), name=f'kernel {kernel}'))\n    \nfig['layout'].update(\n    height = 600, width = 1100, \n    title = f'KDE vs PDF, {N} points, bandwidth {bandwidth}', \n    font=dict(size=16, color='black')\n)\n\n#fig.show()\n#fig.data = []\n#fig.layout = {}\n;\n\n\n''\n\n\nKDE is a neighbor-based approach to density estimation. Another popular approach is using mixture models such as Gaussian Mixtures."
  },
  {
    "objectID": "posts/topic_statistics/2_prob_basics/index.html#probability-spaces",
    "href": "posts/topic_statistics/2_prob_basics/index.html#probability-spaces",
    "title": "Probability theory basics",
    "section": "Probability spaces",
    "text": "Probability spaces\nTODO"
  }
]